{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdYaK1goGQqyyAADFPoWuk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumalatha456/holiday_list/blob/main/NLP_PROJECT_099.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bl7Qp_3NitC3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.Correct the Search Query\n",
        "### Explanation: Here is a basic implementation using Python, focusing on spell correction using edit distance and a predefined corpus of words. This code uses zlib for compression and pickle for serialization, suitable for building an offline model.\n"
      ],
      "metadata": {
        "id": "UFvCuEXMiznU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pickle\n",
        "import zlib\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# Build corpus from a sample dictionary (you can enhance it with more words)\n",
        "words = \"\"\"going to china who was the first president of india winner of the match food in america\"\"\"\n",
        "\n",
        "\n",
        "def words_list(text):\n",
        "    return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "# Create a frequency dictionary from the words\n",
        "WORDS = Counter(words_list(words))\n",
        "\n",
        "# Compress and save the dictionary to a file\n",
        "with open('compressed_dict.pkl', 'wb') as f:\n",
        "    compressed = zlib.compress(pickle.dumps(WORDS))\n",
        "    f.write(compressed)\n",
        "\n",
        "# Load the dictionary from the file\n",
        "def load_dictionary():\n",
        "    with open('compressed_dict.pkl', 'rb') as f:\n",
        "        return pickle.loads(zlib.decompress(f.read()))\n",
        "\n",
        "# Generate all words with an edit distance of 1\n",
        "def edit_distance_one(word):\n",
        "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "\n",
        "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "    deletes = [L + R[1:] for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
        "    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
        "    inserts = [L + c + R for L, R in splits for c in letters]\n",
        "\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "# Filter words that are in the dictionary\n",
        "def known(words, dictionary):\n",
        "    return set(w for w in words if w in dictionary)\n",
        "\n",
        "# Generate candidates for the correction\n",
        "def candidates(word, dictionary):\n",
        "    return (\n",
        "        known([word], dictionary) or\n",
        "        known(edit_distance_one(word), dictionary) or\n",
        "        [word]\n",
        "    )\n",
        "\n",
        "# Find the best correction for a word\n",
        "def correct_word(word, dictionary):\n",
        "    return max(candidates(word, dictionary), key=dictionary.get)\n",
        "\n",
        "# Correct all words in a query\n",
        "def correct_query(query, dictionary):\n",
        "    return ' '.join(correct_word(word, dictionary) for word in query.split())\n",
        "\n",
        "# Main correction function\n",
        "if __name__ == \"__main__\":\n",
        "    dictionary = load_dictionary()\n",
        "    n = int(input(\"Enter the number of queries: \"))\n",
        "    queries = [input(\"Enter query: \").strip() for _ in range(n)]\n",
        "\n",
        "    for query in queries:\n",
        "        print(correct_query(query, dictionary))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQv2_Jj8i1_D",
        "outputId": "ed792668-7eb2-4d62-cc07-167216075c66"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of queries: 2\n",
            "Enter query: giong to chnia\n",
            "Enter query: firzt preisdent of india\n",
            "going to china\n",
            "first president of india\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.Deterministic Url and HashTag Segmentation\n",
        "### Explanation: This approach aims to find the most likely and meaningful segmentation of the input strings based on the provided dictionary of words and the constraint of selecting the longest valid tokens from the left.\n"
      ],
      "metadata": {
        "id": "k0nvL7ogjlTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Upload the dictionary file\n",
        "uploaded = files.upload()\n",
        "with open(\"words.txt\", \"r\") as file:\n",
        "    dictionary = set(word.strip().lower() for word in file.readlines())\n",
        "\n",
        "# Check if a string is a number\n",
        "def is_number(s):\n",
        "    try:\n",
        "        float(s)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "# Tokenize input string based on dictionary\n",
        "def tokenize(input_string, dictionary):\n",
        "    length = len(input_string)\n",
        "    dp = [None] * (length + 1)\n",
        "    dp[0] = []  # Base case: empty string\n",
        "\n",
        "    for i in range(1, length + 1):\n",
        "        for j in range(i):\n",
        "            word = input_string[j:i]\n",
        "            if (word in dictionary or is_number(word)) and dp[j] is not None:\n",
        "                dp[i] = dp[j] + [word] if dp[i] is None or len(dp[j] + [word]) > len(dp[i]) else dp[i]\n",
        "    return dp[length] if dp[length] is not None else [input_string]\n",
        "\n",
        "# Main function to read input and display tokenized output\n",
        "def main():\n",
        "    num_test_cases = int(input(\"Enter number of test cases: \"))\n",
        "    for _ in range(num_test_cases):\n",
        "        input_string = input(\"Enter input string: \").strip().lower()\n",
        "\n",
        "        # Preprocess input for domain names and hashtags\n",
        "        if input_string.startswith(\"www.\"):\n",
        "            input_string = input_string[4:].rsplit(\".\", 1)[0]\n",
        "        elif input_string.startswith(\"#\"):\n",
        "            input_string = input_string[1:]\n",
        "\n",
        "        tokens = tokenize(input_string, dictionary)\n",
        "        print(\"Segmentation:\", ' '.join(tokens))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "6mTYrJNxkBGi",
        "outputId": "fb4c28a6-eb46-43cd-8f9e-c46afb551350"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cc2defba-d67e-4d60-bc52-4591021b2aff\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-cc2defba-d67e-4d60-bc52-4591021b2aff\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving words.txt to words.txt\n",
            "Enter number of test cases: 2\n",
            "Enter input string:  thisisatest\n",
            "Segmentation: thisisatest\n",
            "Enter input string:  example for\n",
            "Segmentation: example for\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.Disambiguation: Mouse vs Mouse\n",
        "### Explanation: This code provides a basic framework for classifying the usage of the word \"mouse\" in a sentence. You can further improve the accuracy by:\n",
        "### Expanding the Training Data: Use a larger and more diverse dataset of sentences.\n",
        "### Experimenting with Different Classifiers: Try other machine learning models like Support Vector Machines (SVM) or Random Forests.\n",
        "### Using Word Embeddings: Consider using word embeddings like Word2Vec or GloVe to capture semantic relationships between words.\n"
      ],
      "metadata": {
        "id": "iFyJbFGNrA46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Training data (sample corpus)\n",
        "training_sentences = [\n",
        "    \"The complete mouse reference genome was sequenced in 2002.\",\n",
        "    \"Tail length varies according to the environmental temperature of the mouse during postnatal development.\",\n",
        "    \"A mouse is an input device.\",\n",
        "    \"Many mice have a pink tail.\",\n",
        "    \"The mouse pointer on the screen helps in navigation.\",\n",
        "    \"A rodent like a mouse has sharp teeth.\",\n",
        "    \"The mouse was connected to the computer using a USB port.\",\n",
        "    \"The house was infested with mice.\",\n",
        "    \"Computer users often prefer a wireless mouse.\"\n",
        "]\n",
        "\n",
        "# Labels corresponding to the training sentences\n",
        "labels = [\n",
        "    \"animal\",\n",
        "    \"animal\",\n",
        "    \"computer-mouse\",\n",
        "    \"animal\",\n",
        "    \"computer-mouse\",\n",
        "    \"animal\",\n",
        "    \"computer-mouse\",\n",
        "    \"animal\",\n",
        "    \"computer-mouse\"\n",
        "]\n",
        "\n",
        "# Vectorize the training sentences\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform(training_sentences)\n",
        "\n",
        "# Create and train the Naive Bayes classifier\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(X_train, labels)\n",
        "\n",
        "# Function to predict the type of \"mouse\"\n",
        "def predict_mouse_type(sentence):\n",
        "    \"\"\"\n",
        "    Predicts whether the 'mouse' in the sentence refers to an animal or a computer mouse.\n",
        "\n",
        "    Args:\n",
        "        sentence: The input sentence.\n",
        "\n",
        "    Returns:\n",
        "        \"animal\" or \"computer-mouse\"\n",
        "    \"\"\"\n",
        "    vectorized_sentence = vectorizer.transform([sentence])\n",
        "    prediction = classifier.predict(vectorized_sentence)[0]\n",
        "    return prediction\n",
        "\n",
        "# Get number of test cases\n",
        "num_test_cases = int(input(\"Enter the number of test cases: \"))\n",
        "\n",
        "# Process each test case\n",
        "for _ in range(num_test_cases):\n",
        "    sentence = input(\"Enter a sentence: \")\n",
        "    prediction = predict_mouse_type(sentence)\n",
        "    print(prediction)\n",
        "\n",
        "# Optionally, save the trained model for later use\n",
        "with open('mouse_classifier.pkl', 'wb') as f:\n",
        "    pickle.dump((vectorizer, classifier), f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQATDhf_jpn1",
        "outputId": "bbb162f3-b440-4e99-c09b-e1babaf3a3ef"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of test cases: 3\n",
            "Enter a sentence: The mouse has a long tail.\n",
            "animal\n",
            "Enter a sentence: I connected the mouse to my laptop\n",
            "computer-mouse\n",
            "Enter a sentence:  The house was infested with mice.\n",
            "animal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.Language Detection\n",
        "### Explanation: This function loads the pre-trained model from a serialized file.\n",
        "### It takes a text snippet as input, normalizes it to ASCII, and converts it into a TF- IDF vector using the loaded vectorizer.\n",
        "### The function then uses the trained classifier to predict the language of the snippet based on the extracted features.\n"
      ],
      "metadata": {
        "id": "FOFU0__5rbPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import unicodedata\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "\n",
        "def normalize_to_ascii(text):\n",
        "    \"\"\"Remove non-ASCII characters and normalize text.\"\"\"\n",
        "    return unicodedata.normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
        "\n",
        "\n",
        "# Step 1: Training Data\n",
        "training_texts = {\n",
        "    \"English\": [\n",
        "        \"The quick brown fox jumps over the lazy dog.\",\n",
        "        \"Rip Van Winkle is a story set in the years before the American Revolutionary War.\",\n",
        "        \"Hello, how are you today?\",\n",
        "        \"It is a wonderful day to learn something new.\"\n",
        "    ],\n",
        "    \"French\": [\n",
        "        \"Le renard brun rapide saute par-dessus le chien paresseux.\",\n",
        "        \"La revolution francaise a marque une periode importante de l'histoire.\",\n",
        "        \"Bonjour, comment ça va?\",\n",
        "        \"Il est temps de découvrir de nouvelles choses.\"\n",
        "    ],\n",
        "    \"German\": [\n",
        "        \"Der schnelle braune Fuchs springt über den faulen Hund.\",\n",
        "        \"Die deutsche Wiedervereinigung war ein historisches Ereignis.\",\n",
        "        \"Hallo, wie geht es dir?\",\n",
        "        \"Es ist ein wunderbarer Tag, um etwas Neues zu lernen.\"\n",
        "    ],\n",
        "    \"Spanish\": [\n",
        "        \"El rapido zorro marron salta sobre el perro perezoso.\",\n",
        "        \"La Revolucion Espanola fue un momento clave en la historia.\",\n",
        "        \"Hola, ¿cómo estás?\",\n",
        "        \"Es un gran día para aprender algo nuevo.\"\n",
        "    ],\n",
        "}\n",
        "\n",
        "\n",
        "# Normalize training data to ASCII\n",
        "labels = []\n",
        "texts = []\n",
        "\n",
        "for language, samples in training_texts.items():\n",
        "    labels.extend([language] * len(samples))\n",
        "    texts.extend([normalize_to_ascii(sample) for sample in samples])\n",
        "\n",
        "\n",
        "# Step 2: Preprocessing and Feature Extraction\n",
        "vectorizer = TfidfVectorizer(ngram_range=(2, 4), analyzer=\"char\")\n",
        "X_train = vectorizer.fit_transform(texts)\n",
        "\n",
        "\n",
        "# Step 3: Train the Model\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(X_train, labels)\n",
        "\n",
        "\n",
        "# Step 4: Serialize the Model\n",
        "with open(\"language_model.pkl\", \"wb\") as model_file:\n",
        "    pickle.dump((vectorizer, classifier), model_file)\n",
        "\n",
        "\n",
        "# Step 5: Language Detection Function\n",
        "def detect_language(snippet):\n",
        "    with open(\"language_model.pkl\", \"rb\") as model_file:\n",
        "        vectorizer, classifier = pickle.load(model_file)\n",
        "\n",
        "    # Normalize snippet to ASCII\n",
        "    snippet = normalize_to_ascii(snippet)\n",
        "    X_test = vectorizer.transform([snippet])\n",
        "    prediction = classifier.predict(X_test)\n",
        "    return prediction[0]\n",
        "\n",
        "\n",
        "# Input Processing (Single-line input)\n",
        "if __name__ == \"__main__\":\n",
        "    # Get user input\n",
        "    snippet = input(\"Enter a text snippet to detect the language: \")\n",
        "\n",
        "    # Predict and Output\n",
        "    detected_language = detect_language(snippet.strip())\n",
        "    print(f\"Detected language: {detected_language}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YgVOLFb3lMOs",
        "outputId": "5c9b06f0-2b01-4016-e9cf-c51038f47018"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a text snippet to detect the language: Kami wa zen'nou de, subete o mimamotteimasu.Nihon de wa Shintou ga dentou-teki na shinkou desu\n",
            "Detected language: French\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.The Missing Apostrophes\n",
        "### Explanation Apostrophe Handling: The code defines a function restore_apostrophes that iterates through each word in the input text. It uses a combination of explicit checks for common contractions (e.g., \"don't,\" \"can't,\" \"I've\") and a regular expression to handle possessive nouns (e.g., \"cat's,\" \"dog's\") to restore apostrophes where appropriate.\n"
      ],
      "metadata": {
        "id": "zxbmPEC7r8dC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Function to handle apostrophes for contractions and possessives\n",
        "def restore_apostrophes(text):\n",
        "    restored_text = []\n",
        "    words = text.split()\n",
        "\n",
        "    for word in words:\n",
        "        lower_word = word.lower()\n",
        "\n",
        "        # Handle contractions\n",
        "        if lower_word == \"dont\":\n",
        "            restored_text.append(\"don't\")\n",
        "        elif lower_word == \"wont\":\n",
        "            restored_text.append(\"won't\")\n",
        "        elif lower_word == \"cant\":\n",
        "            restored_text.append(\"can't\")\n",
        "        elif lower_word == \"isnt\":\n",
        "            restored_text.append(\"isn't\")\n",
        "        elif lower_word == \"arent\":\n",
        "            restored_text.append(\"aren't\")\n",
        "        elif lower_word == \"wasnt\":\n",
        "            restored_text.append(\"wasn't\")\n",
        "        elif lower_word == \"werent\":\n",
        "            restored_text.append(\"weren't\")\n",
        "        elif lower_word == \"hasnt\":\n",
        "            restored_text.append(\"hasn't\")\n",
        "        elif lower_word == \"havent\":\n",
        "            restored_text.append(\"haven't\")\n",
        "        elif lower_word == \"hadnt\":\n",
        "            restored_text.append(\"hadn't\")\n",
        "        elif lower_word == \"didnt\":\n",
        "            restored_text.append(\"didn't\")\n",
        "        elif lower_word == \"ive\":\n",
        "            restored_text.append(\"I've\")\n",
        "        elif lower_word == \"were\":\n",
        "            restored_text.append(\"we're\")\n",
        "        elif lower_word == \"i\":\n",
        "            restored_text.append(\"I\")\n",
        "        elif lower_word == \"id\":\n",
        "            restored_text.append(\"I'd\")\n",
        "        elif lower_word == \"youve\":\n",
        "            restored_text.append(\"you've\")\n",
        "        elif lower_word == \"hes\":\n",
        "            restored_text.append(\"he's\")\n",
        "        elif lower_word == \"shes\":\n",
        "            restored_text.append(\"she's\")\n",
        "        elif lower_word == \"its\":\n",
        "            restored_text.append(\"it's\")\n",
        "        elif lower_word == \"were\":\n",
        "            restored_text.append(\"we're\")\n",
        "\n",
        "        # Handle possessives (only add 's when it makes sense)\n",
        "        elif re.match(r'\\w+s$', word) and lower_word not in [\"its\", \"hers\", \"ours\", \"yours\", \"theirs\"]:\n",
        "            restored_text.append(re.sub(r\"s$\", \"'s\", word))\n",
        "\n",
        "        # For normal words that don't need apostrophes, keep them as is\n",
        "        else:\n",
        "            restored_text.append(word)\n",
        "\n",
        "    return \" \".join(restored_text)\n",
        "\n",
        "\n",
        "# Input\n",
        "input_text = \"\"\"At a news conference Thursday at the Russian manned-space facility in Baikonur, Kazakhstan, Kornienko said \"we will be missing nature, we will be missing landscapes, woods.\" He admitted that on his previous trip into space in 2010 \"I even asked our psychological support folks to send me a calendar with photographs of nature, of rivers, of woods, of lakes.\"\n",
        "Kelly was asked if hed miss his twin brother Mark, who also was an astronaut.\n",
        "\n",
        "\"Were used to this kind of thing,\" he said. \"Ive gone longer without seeing him and it was great.\"\n",
        "The mission wont be the longest time that a human has spent in space - four Russians spent a year or more aboard the Soviet-built Mir space station in the 1990s.\n",
        "SCI Astronaut Twins\n",
        "Scott Kelly (left) was asked Thursday if hed miss his twin brother, Mark, who also was an astronaut. Were used to this kind of thing, he said. Ive gone longer without seeing him and it was great. (NASA/Associated Press)\n",
        "\"The last time we had such a long duration flight was almost 20 years and of course al{-truncated-}\"\"\"\n",
        "\n",
        "# Restore apostrophes\n",
        "output_text = restore_apostrophes(input_text)\n",
        "print(output_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqpTWboojwGF",
        "outputId": "4f9c9929-ea59-468a-a4ec-9010265fa0c6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "At a new's conference Thursday at the Russian manned-space facility in Baikonur, Kazakhstan, Kornienko said \"we will be missing nature, we will be missing landscapes, woods.\" He admitted that on hi's previou's trip into space in 2010 \"I even asked our psychological support folk's to send me a calendar with photograph's of nature, of rivers, of woods, of lakes.\" Kelly wa's asked if hed mis's hi's twin brother Mark, who also wa's an astronaut. \"Were used to thi's kind of thing,\" he said. \"Ive gone longer without seeing him and it wa's great.\" The mission won't be the longest time that a human ha's spent in space - four Russian's spent a year or more aboard the Soviet-built Mir space station in the 1990s. SCI Astronaut Twin's Scott Kelly (left) wa's asked Thursday if hed mis's hi's twin brother, Mark, who also wa's an astronaut. we're used to thi's kind of thing, he said. I've gone longer without seeing him and it wa's great. (NASA/Associated Press) \"The last time we had such a long duration flight wa's almost 20 year's and of course al{-truncated-}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.Segment the Twitter Hashtags\n",
        "### Explanation: Tokenization with Dynamic Programming: The segment_hashtag function uses dynamic programming to break down the hashtag into a sequence of words. It iterates through the hashtag, checking for valid word combinations from a given dictionary and selecting the longest possible valid sequence\n"
      ],
      "metadata": {
        "id": "8o8K_UnesDOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function that segments a single hashtag into words\n",
        "def segment_hashtag(hashtag, word_dict):\n",
        "    n = len(hashtag)\n",
        "    dp = [None] * (n + 1)\n",
        "\n",
        "    dp[0] = []  # Base case: empty string can be segmented as an empty list\n",
        "\n",
        "    # Iterate over the hashtag string\n",
        "    for i in range(1, n + 1):\n",
        "        for j in range(max(0, i - 20), i):  # Limit the length of words checked\n",
        "            word = hashtag[j:i]\n",
        "            if word in word_dict and dp[j] is not None:\n",
        "                dp[i] = dp[j] + [word]\n",
        "                break\n",
        "\n",
        "    return \" \".join(dp[n]) if dp[n] is not None else hashtag\n",
        "\n",
        "\n",
        "# Main function to process input and output results\n",
        "def process_hashtags(num_hashtags, hashtags, word_dict):\n",
        "    result = []\n",
        "    for hashtag in hashtags:\n",
        "        segmented = segment_hashtag(hashtag, word_dict)\n",
        "        result.append(segmented)\n",
        "    return result\n",
        "\n",
        "\n",
        "# Sample dictionary of common words (expand this as needed)\n",
        "word_dict = {\n",
        "    \"we\", \"are\", \"the\", \"people\", \"mention\", \"your\", \"faves\",\n",
        "    \"now\", \"playing\", \"walking\", \"dead\", \"follow\", \"me\"\n",
        "}\n",
        "\n",
        "\n",
        "# Sample input\n",
        "num_hashtags = int(input(\"Enter the number of hashtags: \"))\n",
        "hashtags = [input(f\"Enter hashtag {i + 1}: \").strip() for i in range(num_hashtags)]\n",
        "\n",
        "\n",
        "# Process the hashtags and print the result\n",
        "segmented_hashtags = process_hashtags(num_hashtags, hashtags, word_dict)\n",
        "for segmented in segmented_hashtags:\n",
        "    print(segmented)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFld98DpsAb3",
        "outputId": "570a4584-029b-45ba-f73d-3e48a2afcf54"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of hashtags: 3\n",
            "Enter hashtag 1: wearethepeople\n",
            "Enter hashtag 2: mentionyourfaves\n",
            "Enter hashtag 3: playingwalkingdead\n",
            "we are the people\n",
            "mention your faves\n",
            "playing walking dead\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.Expand the Acronyms\n",
        "### Explanation: Acronym Extraction: The code extracts acronyms and their potential expansions from a given set of text snippets by identifying uppercase words within parentheses and searching for preceding phrases. It also attempts to extract acronyms not explicitly defined in parentheses by analyzing the surrounding context.\n"
      ],
      "metadata": {
        "id": "_DlcGm-SsUHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_acronyms_and_expansions(snippets):\n",
        "    \"\"\"\n",
        "    Extract acronyms and their expansions from the provided snippets.\n",
        "    \"\"\"\n",
        "    acronym_dict = {}\n",
        "\n",
        "    for snippet in snippets:\n",
        "        print(f\"Processing snippet: {snippet}\")\n",
        "\n",
        "        # 1. Find all potential acronyms (uppercase words typically enclosed in parentheses)\n",
        "        matches = re.findall(r'\\(([^)]+)\\)', snippet)  # Capture everything inside parentheses\n",
        "        print(f\"Found acronyms in parentheses: {matches}\")\n",
        "\n",
        "        for match in matches:\n",
        "            # Split the match by spaces to capture the acronym and the expansion\n",
        "            acronym_expansion = match.split(' ', 1)\n",
        "            if len(acronym_expansion) == 2:  # If we have both acronym and expansion\n",
        "                acronym = acronym_expansion[0].strip()\n",
        "                expansion = acronym_expansion[1].strip()\n",
        "                acronym_dict[acronym] = expansion\n",
        "                print(f\"Captured acronym-expansion pair: {acronym} -> {expansion}\")\n",
        "\n",
        "        # 2. Handle acronyms not in parentheses but defined explicitly (case-sensitive)\n",
        "        words = snippet.split()\n",
        "        for i, word in enumerate(words):\n",
        "            if word.isupper() and len(word) > 1:  # Likely an acronym\n",
        "                if word not in acronym_dict:\n",
        "                    # Try to extract its expansion from the surrounding context\n",
        "                    preceding_context = \" \".join(words[max(0, i-5):i])\n",
        "                    if preceding_context:\n",
        "                        acronym_dict[word] = preceding_context.strip()\n",
        "                        print(f\"Captured explicit expansion for {word}: {preceding_context}\")\n",
        "\n",
        "    return acronym_dict\n",
        "\n",
        "\n",
        "def process_tests(acronym_dict, tests):\n",
        "    \"\"\"\n",
        "    Process test acronyms and return their expansions.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for test in tests:\n",
        "        # Normalize the test acronym (case insensitive)\n",
        "        expansion = acronym_dict.get(test.upper(), \"Not Found\")\n",
        "        print(f\"Processing test acronym: {test}, found expansion: {expansion}\")\n",
        "        results.append(expansion)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Read input\n",
        "    n = int(input(\"Enter number of snippets: \").strip())\n",
        "\n",
        "    snippets = [input(f\"Enter snippet {i + 1}: \").strip() for i in range(n)]\n",
        "    tests = [input(f\"Enter test acronym {i + 1}: \").strip() for i in range(n)]\n",
        "\n",
        "    # Extract acronyms and expansions\n",
        "    acronym_dict = extract_acronyms_and_expansions(snippets)\n",
        "\n",
        "    # Process test queries\n",
        "    results = process_tests(acronym_dict, tests)\n",
        "\n",
        "    # Output results\n",
        "    print(\"\\nResults:\")\n",
        "    for result in results:\n",
        "        print(result)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sK6XKgCysKMA",
        "outputId": "9a444abe-517b-4e76-ee14-c957127ea375"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter number of snippets: 1\n",
            "Enter snippet 1: The system of Local Area Network (LAN) allows communication.\n",
            "Enter test acronym 1: LAN\n",
            "Processing snippet: The system of Local Area Network (LAN) allows communication.\n",
            "Found acronyms in parentheses: ['LAN']\n",
            "Captured explicit expansion for (LAN): system of Local Area Network\n",
            "Processing test acronym: LAN, found expansion: Not Found\n",
            "\n",
            "Results:\n",
            "Not Found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.Correct the Search Query\n",
        "#### Explanation: Here is a basic implementation using Python, focusing on spell correction using edit distance and a predefined corpus of words. This code uses zlib for compression and pickle for serialization, suitable for building an offline model.\n"
      ],
      "metadata": {
        "id": "4ar8iQ-syiIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pickle\n",
        "import zlib\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# Build corpus from a sample dictionary (you can enhance it with more words)\n",
        "words = \"\"\"going to china who was the first president of india winner of the match food in america\"\"\"\n",
        "\n",
        "# Function to return a list of words from the text\n",
        "def words_list(text):\n",
        "    return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "# Count words in the corpus\n",
        "WORDS = Counter(words_list(words))\n",
        "\n",
        "# Compression for large wordlist\n",
        "with open('compressed_dict.pkl', 'wb') as f:\n",
        "    compressed = zlib.compress(pickle.dumps(WORDS))\n",
        "    f.write(compressed)\n",
        "\n",
        "# Load dictionary in memory\n",
        "def load_dictionary():\n",
        "    with open('compressed_dict.pkl', 'rb') as f:\n",
        "        return pickle.loads(zlib.decompress(f.read()))\n",
        "\n",
        "# Generate a set of possible words with a single edit distance\n",
        "def edit_distance_one(word):\n",
        "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "\n",
        "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
        "    deletes = [L + R[1:] for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
        "    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
        "    inserts = [L + c + R for L, R in splits for c in letters]\n",
        "\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "# Known words in the dictionary\n",
        "def known(words, dictionary):\n",
        "    return set(w for w in words if w in dictionary)\n",
        "\n",
        "# Candidates for correction based on edit distance\n",
        "def candidates(word, dictionary):\n",
        "    return (known([word], dictionary) or known(edit_distance_one(word), dictionary) or [word])\n",
        "\n",
        "# Correct a single word\n",
        "def correct_word(word, dictionary):\n",
        "    return max(candidates(word, dictionary), key=dictionary.get)\n",
        "\n",
        "# Correct a whole query\n",
        "def correct_query(query, dictionary):\n",
        "    return ' '.join(correct_word(word, dictionary) for word in query.split())\n",
        "\n",
        "# Main correction function\n",
        "if __name__ == \"__main__\":\n",
        "    # Load the dictionary\n",
        "    dictionary = load_dictionary()\n",
        "\n",
        "    # Input number of queries\n",
        "    n = int(input(\"Enter the number of queries: \").strip())\n",
        "\n",
        "    # Input queries\n",
        "    queries = [input(f\"Enter query {i+1}: \").strip() for i in range(n)]\n",
        "\n",
        "    # Process each query and correct it\n",
        "    for query in queries:\n",
        "        print(correct_query(query, dictionary))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dwgWWkMsbF7",
        "outputId": "d826d21c-e057-4d47-cbf6-6943c877afec"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of queries: 2\n",
            "Enter query 1: goin to china\n",
            "Enter query 2: winer of the match\n",
            "going to china\n",
            "winner of the match\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.A Text-Processing Warmup\n",
        "### Explanation: Article and Date Counting: The code defines a function count_articles_and_dates that takes a text fragment as input. It first normalizes the text to lowercase for case-insensitive article counting. Then, it uses regular expressions to count occurrences of the definite and indefinite articles (\"a,\" \"an,\" \"the\") and identify valid dates in various formats (e.g., \"DD Month YYYY,\" \"Month DD, YYYY,\" etc.)\n"
      ],
      "metadata": {
        "id": "Jlu86wjsdtmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def count_articles_and_dates(fragment):\n",
        "    \"\"\"\n",
        "    Count occurrences of 'a', 'an', 'the', and valid dates in a given text fragment.\n",
        "    \"\"\"\n",
        "    # Normalize text for article counting\n",
        "    lower_fragment = fragment.lower()\n",
        "\n",
        "    # Count articles (simplified to handle punctuation better)\n",
        "    a_count = len(re.findall(r'\\ba\\b', lower_fragment))\n",
        "    an_count = len(re.findall(r'\\ban\\b', lower_fragment))\n",
        "    the_count = len(re.findall(r'\\bthe\\b', lower_fragment))\n",
        "\n",
        "    # Debugging: Print counts of articles\n",
        "    print(f\"Fragment: {fragment}\")\n",
        "    print(f\"a_count: {a_count}, an_count: {an_count}, the_count: {the_count}\")\n",
        "\n",
        "    # Identify valid dates\n",
        "    date_patterns = [\n",
        "        # Day Month Year (e.g., 5th January 2023)\n",
        "        r'\\b\\d{1,2}(?:st|nd|rd|th)?(?:\\s+of)?\\s+(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{2,4}\\b',\n",
        "\n",
        "        # Month Day Year (e.g., January 15, 2021)\n",
        "        r'\\b(January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2}(?:st|nd|rd)?(?:,?)?\\s+\\d{2,4}\\b',\n",
        "\n",
        "        # Day/Month/Year (e.g., 12/02/2021)\n",
        "        r'\\b\\d{1,2}/\\d{1,2}/\\d{2,4}\\b',  # Day/Month/Year\n",
        "\n",
        "        # ISO format: Year-Month-Day (e.g., 2021-12-15)\n",
        "        r'\\b\\d{4}-\\d{2}-\\d{2}\\b'  # ISO format: Year-Month-Day\n",
        "    ]\n",
        "\n",
        "    # Combine all date patterns\n",
        "    date_regex = '|'.join(date_patterns)\n",
        "    dates = re.findall(date_regex, fragment, re.IGNORECASE)\n",
        "\n",
        "    # Debugging: Print found dates\n",
        "    print(f\"Found dates: {dates}\")\n",
        "\n",
        "    date_count = len(dates)\n",
        "\n",
        "    return a_count, an_count, the_count, date_count\n",
        "\n",
        "def main():\n",
        "    # Directly simulate input for testing (use your own test cases here)\n",
        "    data = \"\"\"2\n",
        "I went to the park on 5th January 2023.\n",
        "She arrived on 12/02/2021 and met John on January 15, 2021.\"\"\"\n",
        "\n",
        "    data = data.strip().split(\"\\n\")\n",
        "\n",
        "    # Proceed if data is not empty\n",
        "    if not data:\n",
        "        print(\"Error: No input data provided\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        t = int(data[0].strip())  # Number of test cases\n",
        "    except ValueError:\n",
        "        print(\"Error: Invalid number of test cases\")\n",
        "        return\n",
        "\n",
        "    fragments = data[1:]  # Remaining lines contain the fragments\n",
        "\n",
        "    if len(fragments) != t:\n",
        "        print(f\"Error: Expected {t} fragments, but got {len(fragments)}\")\n",
        "        return\n",
        "\n",
        "    results = []\n",
        "    for i in range(t):\n",
        "        fragment = fragments[i].strip()  # Count articles and dates\n",
        "        a_count, an_count, the_count, date_count = count_articles_and_dates(fragment)\n",
        "        results.append(f\"{a_count}\\n{an_count}\\n{the_count}\\n{date_count}\")\n",
        "\n",
        "    # Output results\n",
        "    print(\"\\n\".join(results))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UORRy0ryd0l7",
        "outputId": "cff55773-0841-4a2c-e39a-30666b7f5deb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fragment: I went to the park on 5th January 2023.\n",
            "a_count: 0, an_count: 0, the_count: 1\n",
            "Found dates: [('January', '')]\n",
            "Fragment: She arrived on 12/02/2021 and met John on January 15, 2021.\n",
            "a_count: 0, an_count: 0, the_count: 0\n",
            "Found dates: [('', ''), ('', 'January')]\n",
            "0\n",
            "0\n",
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Who is it?\n",
        "### Explanation: Pronoun Identification and Entity Matching: The code first finds all pronouns (words enclosed in double backslashes) and their positions in the text. It then cleans the text by removing the backslashes. Next, it iterates through each pronounand searches for the closest matching entity (from a provided list) that appears before the pronoun in the text.\n"
      ],
      "metadata": {
        "id": "3SjKuYNnd8UT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def resolve_pronouns(text, entities):\n",
        "    \"\"\"\n",
        "    Resolves pronouns in the text based on provided entities.\n",
        "    \"\"\"\n",
        "    # Extract all pronouns and their positions\n",
        "    pronoun_pattern = r'\\b(\\w+)\\b'  # Updated pattern to match any word\n",
        "    pronouns = [(match.group(1), match.start()) for match in re.finditer(pronoun_pattern, text)]\n",
        "\n",
        "    # Clean the text by removing ** markers (if any)\n",
        "    clean_text = re.sub(r'\\*\\*(\\w+)\\*\\*', r'\\1', text)\n",
        "\n",
        "    # Initialize a list to store the resolved entities\n",
        "    resolved = []\n",
        "\n",
        "    # For each pronoun, find the corresponding entity\n",
        "    for pronoun, pos in pronouns:\n",
        "        closest_entity = None\n",
        "        closest_distance = float('inf')\n",
        "\n",
        "        # Iterate through all entities to find the best match for the pronoun\n",
        "        for entity in entities:\n",
        "            entity_pos = clean_text.rfind(entity, 0, pos)  # Find the last occurrence of the entity before the pronoun\n",
        "            if entity_pos != -1:\n",
        "                distance = pos - (entity_pos + len(entity))\n",
        "                if distance < closest_distance:\n",
        "                    closest_distance = distance\n",
        "                    closest_entity = entity\n",
        "\n",
        "        # Append the resolved entity to the list\n",
        "        if closest_entity:\n",
        "            resolved.append(closest_entity)\n",
        "\n",
        "    return resolved\n",
        "\n",
        "def main():\n",
        "    # Read input interactively\n",
        "    print(\"Enter number of text snippets:\")\n",
        "    n = int(input().strip())  # Read number of snippets\n",
        "\n",
        "    print(f\"Enter {n} lines of text:\")\n",
        "    text_snippet = \" \".join(input().strip() for _ in range(n))  # Read text snippet lines\n",
        "\n",
        "    print(\"Enter entities (separated by ';'):\")\n",
        "    entities = [e.strip() for e in input().strip().split(';')]  # Read list of entities\n",
        "\n",
        "    # Resolve pronouns\n",
        "    result = resolve_pronouns(text_snippet, entities)\n",
        "\n",
        "    # Output the resolved entities\n",
        "    for entity in result:\n",
        "        print(entity)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOHBf0GaeB_H",
        "outputId": "58709442-350b-43da-aca9-a779d24f27e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter number of text snippets:\n",
            "2\n",
            "Enter 2 lines of text:\n",
            "John and Mary went to the park.\n",
            "She gave him a gift.\n",
            "Enter entities (separated by ';'):\n",
            "John; Mary\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jFavwZH6eEcx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}